<html>
    <head>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">

        <script src="67960BlogPost_files/jquery.min.js"></script>
        <script>
        </script>

        <link rel="shortcut icon" href="https://web.mit.edu/phillipi/www/blog_template/images/icon.ico">
        <style type="text/css">
            body {
                background-color: #f5f9ff;
            }

            /* Hide both math displays initially, will display based on JS detection */
            .mathjax-mobile, .mathml-non-mobile { display: none; }

            /* Show the MathML content by default on non-mobile devices */
            .show-mathml .mathml-non-mobile { display: block; }
            .show-mathjax .mathjax-mobile { display: block; }

            .content-margin-container {
                display: flex;
                width: 100%; /* Ensure the container is full width */
                justify-content: left; /* Horizontally centers the children in the container */
                align-items: center;  /* Vertically centers the children in the container */
            }

            .main-content-block {
                width: 70%; /* Change this percentage as needed */
                max-width: 1100px; /* Optional: Maximum width */
                background-color: #fff;
                border-left: 1px solid #DDD;
                border-right: 1px solid #DDD;
                padding: 8px 8px 8px 8px;
                font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
                font-size: 18px;
                line-height: 24px;
            }

            .margin-left-block {
                font-size: 14px;
                width: 15%; /* Change this percentage as needed */
                max-width: 130px; /* Optional: Maximum width */
                position: relative;
                margin-left: 10px;
                text-align: left;
                font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
                padding: 5px;
            }

            .margin-right-block {
                font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
                font-size: 14px;
                width: 25%; /* Change this percentage as needed */
                max-width: 256px; /* Optional: Maximum width */
                position: relative;
                text-align: left;
                padding: 10px;  /* Optional: Adds padding inside the caption */
            }

            img {
                max-width: 100%; /* Make sure it fits inside the container */
                height: auto;
                display: block;
                margin: auto;
            }

            .my-video {
                max-width: 100%; /* Make sure it fits inside the container */
                height: auto;
                display: block;
                margin: auto;
            }

            /* Hide both video displays initially, will display based on JS detection */
            .vid-mobile, .vid-non-mobile { display: none; }

            /* Show the video content by default on non-mobile devices */
            .show-vid-mobile .vid-mobile { display: block; }
            .show-vid-non-mobile .vid-non-mobile { display: block; }

            a:link,a:visited {
                color: #0e7862; /*#1367a7;*/
                text-decoration: none;
            }

            a:hover {
                color: #24b597; /*#208799;*/
            }

            h1 {
                font-size: 18px;
                margin-top: 4px;
                margin-bottom: 10px;
            }

            table.header {
                font-weight: 300;
                font-size: 17px;
                flex-grow: 1;
                width: 70%;
                max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
            }

            table td, table td * {
                vertical-align: middle;
                position: relative;
            }

            table.paper-code-tab {
                flex-shrink: 0;
                margin-left: 8px;
                margin-top: 8px;
                padding: 0px 0px 0px 8px;
                width: 290px;
                height: 150px;
            }

            .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
                box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
                margin-top: 5px;
                margin-left: 10px;
                margin-right: 30px;
                margin-bottom: 5px;
            }

            hr {
                height: 1px; /* Sets the height of the line to 1 pixel */
                border: none; /* Removes the default border */
                background-color: #DDD; /* Sets the line color to black */
            }

            div.hypothesis {
                width: 80%;
                background-color: #EEE;
                border: 1px solid black;
                border-radius: 10px;
                -moz-border-radius: 10px;
                -webkit-border-radius: 10px;
                font-family: Courier;
                font-size: 18px;
                text-align: center;
                margin: auto;
                padding: 16px 16px 16px 16px;
            }

            div.citation {
                font-size: 0.8em;
                background-color:#fff;
                padding: 10px;
                height: 200px;
            }

            .fade-in-inline {
                position: absolute;
                text-align: center;
                margin: auto;
                -webkit-mask-image: linear-gradient(to right,
                transparent 0%,
                transparent 40%,
                black 50%,
                black 90%,
                transparent 100%);
                mask-image: linear-gradient(to right,
                transparent 0%,
                transparent 40%,
                black 50%,
                black 90%,
                transparent 100%);
                -webkit-mask-size: 8000% 100%;
                mask-size: 8000% 100%;
                animation-name: sweepMask;
                animation-duration: 4s;
                animation-iteration-count: infinite;
                animation-timing-function: linear;
                animation-delay: -1s;
            }

            .fade-in2-inline {
                animation-delay: 1s;
            }

            .inline-div {
                position: relative;
                display: inline-block; /* Makes both the div and paragraph inline-block elements */
                vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
                width: 50px; /* Optional: Adds space between the div and the paragraph */
            }

            .line-break {
                height: 20px;
            }

            .side-by-side-container {
                display: flex;
                flex-direction: row;
            }

            .side-by-side-group {
                display: flex;
                flex-direction: column;
                align-items: center;
            }

            .side-by-side-image {
                width: 95%;
                margin-bottom: 20px;
            }

            .caption {
                font-style: italic;
                font-size: 15px;
            }
        </style>

        <title>Oasis-Memory: Improving DiT Temporal Consistency with Memory</title>
        <meta property="og:title" content="Oasis-Memory: Improving DiT Temporal Consistency with Memory">
        <meta charset="UTF-8">
    </head>
    <body>

        <div class="content-margin-container">
            <div class="margin-left-block">
            </div>
            <div class="main-content-block">
                <table class="header" align="left">
                    <tbody><tr>
                        <td colspan="4">
                            <span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">
                                Oasis-Memory: Improving DiT Temporal Consistency with Memory
                            </span>
                        </td>
                    </tr>
                        <tr>
                            <td align="left">
                                <span style="font-size:17px"><a href="https://evanthompson.site">Evan Thompson</a></span>
                            </td>
                            <td align="left">
                                <span style="font-size:17px"><a href="https://github.com/lowtorola">Lowell Torola</a></span>
                            </td>
                            <td align="left">
                                <span style="font-size:17px"><a href="https://github.com/hstennes">Hank Stennes</a></span>
                            </td>
                        </tr><tr>
                            <td colspan="4" align="left"><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
                        </tr>
                    </tbody></table>
            </div>
            <div class="margin-right-block">
            </div>
        </div>

        <div class="content-margin-container" id="intro">
            <div class="margin-left-block">
                <!-- table of contents here -->
                <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
                    <b style="font-size:16px">Outline</b><br><br>
                    <a href="#introduction">Introduction</a><br><br>
                    <a href="#background">Background and Related Work</a><br><br>
                    <a href="#baseline_setup">Baseline Setup</a><br><br>
                    <a href="#experimental_setup">Experimental Setup</a><br><br>
                    <a href="#experimentation_and_results">Experimentation and Results</a><br><br>
                    <a href="#implications_and_limitations">Implications and Limitations</a><br><br>
                    <a href="#citations">Citations</a><br><br>
                </div>
            </div>
            <div class="main-content-block">
                <!--You can embed an image like this:-->
                <img src="67960BlogPost_files/gifs/header.gif" class="side-by-side-image" />
                <div style="width: 100%; display: flex; align-items: center; justify-content: center; text-align: center;">
                    <div style="width: 15%;"></div>
                    <div class="caption">Original</div>
                    <div style="width: 26%;"></div>
                    <div class="caption">Baseline</div>
                    <div style="width: 28%;"></div>
                    <div class="caption">Ours</div>
                    <div style="width: 15%;"></div>
                </div>
            </div>
            <div class="margin-right-block">
            </div>
        </div>

        <!-- -->

        <div class="content-margin-container" id="introduction">
            <div class="margin-left-block">
            </div>
            <div class="main-content-block">
                <h1>Introduction</h1>
                The recent Oasis model uses a diffusion transformer (DiT) to emulate Minecraft in real-time while responding to user
                input <a href="#ref_1">[1]</a>. While this model is not the first of its kind, it made significant advances in performance to allow real-time
                gameplay. However, the model exhibits limited memory over time due to its constrained frame-based history architecture.
                For example, if the player turns around or looks up, it is likely that the landscape surrounding them will change. Our
                goal is to investigate techniques for improving memory in DiTs to increase the temporal consistency of generated video.
                We believe that this improvement is crucial for models like Oasis to be practical in a gaming setting or for other use
                cases. </div>
            <div class="margin-right-block">
            </div>
        </div>

        <!-- -->

        <div class="content-margin-container" id="background">
            <div class="margin-left-block">
            </div>
            <div class="main-content-block">
                <h1>Background and Related Work</h1>
                Oasis itself was inspired by several similar projects such as the DOOM model presented in <i>Diffusion Models Are
                    Real-Time
                    Game Engines</i> <a href="#ref_2">[2]</a>. This model is similar in spirit to Oasis, but differs significantly in implementation in that
                it did
                not use transformers. We investigated this paper for ideas for solving the memory problem, but found that it was not
                directly addressed. This seemed to be less of a problem in this paper since frames in DOOM are more consistent than in
                Minecraft, with the entire game taking place in a similar environment. To gain inspiration for our memory system
                structure, we also referenced the paper World Models <a href="#ref_3">[3]</a>. This paper focuses on building generative models that
                reproduce common reinforcement learning environments. This model includes a variational autoencoder (VAE) that encodes frames, along with
                a recurrent neural network
                memory system that predicts future outputs of the VAE. The next step of the model takes into account both the output of
                the VAE and the memory system’s predicted output. We took some inspiration from this structure when designing our model.
                <div class="line-break"></div>
                We used Oasis as a starting point in developing our model. Most of the structure remains the same aside from the memory
                enhancements we will discuss. However, Oasis did not have open source training code, so this was developed from scratch.
                We used the paper Diffusion Forcing as a reference for how to train diffusion transformers <a href="#ref_4">[4]</a>.
            </div>
            <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
            </div>
        </div>

        <!-- -->

        <div class="content-margin-container" id="baseline_setup">
            <div class="margin-left-block">
            </div>
            <div class="main-content-block">
                <h1>Baseline Setup</h1>
                <div class="side-by-side-group">
                    <img src="67960BlogPost_files/oasis-dit.png" class="side-by-side-image" style="max-width: 500px"/>
                    <div class="caption">Oasis’ autoregressive, transformer-based architecture <a href="#ref_1">[1]</a></div>
                </div>
                <div class="line-break"></div>
                The base Oasis model consists of two primary components: the variational autoencoder (VAE) and the diffusion transformer
                (DiT). The VAE is responsible for learning a compressed representation of an individual frame, whose smaller
                dimensionality aids in the diffusion process. The DiT takes in a series of VAE-encoded frames and a condition vector
                derived from the user input to generate the next frame, which are VAE latents that are subsequently decoded. Diffusion
                sampling is performed using DDIM, which speeds up generation <a href="#ref_9">[9]</a>.
                <div class="line-break"></div>

                In order to create a baseline to compare against, we needed to set up a training environment that would allow us to
                train the baseline model given our compute and time constraints. The original Oasis repository did not provide any code
                or guidance for training, so we devised our own scheme.
                <div class="line-break"></div>

                We first trained the VAE to reconstruct arbitrary frames using a fairly standard self-supervised training process, which
                incorporates MSE reconstruction loss, KL-divergence loss, and LPIPS loss <a href="#ref_5">[5]</a>. We used the same number of model
                parameters as the original Oasis model except for the frame resolution, which we halved from (640x320) to (320x160) for
                faster training. We trained using a large subset of the original dataset <a href="#ref_6">[6]</a> (around 3000 videos) for around 20k
                iterations so that the VAE would generalize fairly well.
                <div class="line-break"></div>
                <div class="line-break"></div>
                <div class="side-by-side-container">
                    <div class="side-by-side-group">
                        <img src="67960BlogPost_files/vae-oasis.png" class="side-by-side-image" />
                        <div class="caption">VAE Reconstruction - Oasis (Right)</div>
                    </div>
                    <div class="side-by-side-group">
                        <img src="67960BlogPost_files/vae-ours.png" class="side-by-side-image" />
                        <div class="caption">VAE Reconstruction - Ours (Right)</div>
                    </div>
                </div>
                <div class="line-break"></div>
                <div class="line-break"></div>

                In order to train the DiT, we needed to overfit on a smaller dataset. The training was too slow per iteration and we
                would need tens of thousands of iterations over the full dataset before we would get meaningfully coherent results. To
                solve this problem, we authored a custom labeled dataset of 44 videos that we could overfit on. To maximize usefulness
                for later benchmarking, we recorded two videos in each biome, totalling 22 biomes, where the player would walk around
                for a few seconds, look up into the sky for a few seconds, and then look back down and continue walking for a few
                seconds. Since we did not have access to the program used to create the original dataset, we manually set the proper
                resolution and game settings, and we used OpenAI’s inverse dynamics model to infer the user input from the recorded
                videos <a href="#ref_6">[6]</a>.
                <div class="line-break"></div>
                <div class="line-break"></div>
                <div class="side-by-side-group">
                    <img src="67960BlogPost_files/gifs/data/data_19_walk.gif" class="side-by-side-image" style="max-width: 500px"/>
                    <div class="caption">Example Dataset Video <a href="#ref_1">[1]</a></div>
                </div>
                <div class="line-break"></div>
                <div class="line-break"></div>

                Using this dataset, we trained the DiT to predict latents of our pretrained VAE. To do this, we closely followed the
                training process outlined in the diffusion forcing paper <a href="#ref_4">[4]</a>. We computed a scaling factor for our VAE latents to ensure
                we sampled our latents from a correctly-parametrized distribution <a href="#ref_7">[7]</a>. Similarly to the VAE, we used the same model
                parameters as the original Oasis model except for the context window length, which we reduced from 32 to 24 for faster
                training. With our dataset, we were able to get coherent results in a reasonable amount of time with around 10k
                iterations.
            </div>
            <div class="margin-right-block">
            </div>
        </div>

        <!-- -->

        <div class="content-margin-container" id="experimental_setup">
            <div class="margin-left-block">
            </div>
            <div class="main-content-block">
                <h1>Experimental Setup</h1>
                <h4>Goals</h4>
                Our baseline model has many visible issues. It quickly diverges from the prompt frames and devolves into other
                environments, and is overall still very noisy. These issues would likely be mitigated with more data and training, but a
                goal of our experiments is to improve consistency even with a smaller number of training iterations. Another fundamental
                issue with the baseline is its lack of memory when looking away from and back to a specific environment. This cannot be
                solved with more training, and we will propose modifications to the original architecture to solve this issue.
                <h4>Proposed Modifications</h4>

                <div class="side-by-side-group">
                    <img src="67960BlogPost_files/oasis-memory.jpg" class="side-by-side-image" style="max-width: 500px"/>
                    <div class="caption">Oasis-Memory architecture. Existing components highlighted beige and new components highlighted green.</div>
                </div>
                <div class="line-break"></div>
                <div class="line-break"></div>

                We propose a modified model, Oasis-Memory, which amends a medium-term memory pipeline to the existing Oasis model.

                <div class="line-break"></div>

                The first step in the memory pipeline is the memory encoder module. It processes a frame from the input sequence and
                generates an encoded memory vector that represents the frame. In our experiments, we tested two different encoder
                implementations. The first is a memory compressor (MC-small) that drastically downsamples and quantizes the input frame,
                then compresses it using a small-bottleneck VAE. The other simply uses the original resolution latents from the primary
                pretrained VAE (MC-large). Both output an embedding that is passed off to the memory bank module.

                <div class="line-break"></div>

                The memory bank stores a fixed-length context of memory encodings. We considered several replacement policies for this
                memory bank, but settled on a FIFO system where a new encoding is pushed to the queue when the cosine difference between
                the encoded frame and the most-recently-added entry is above a certain threshold. With this technique, we only add data
                to the memory bank when the environment changes sufficiently, which allows memory over a much longer context length. For
                each frame in the sequence, the memory bank produces a ‘snapshot’ of memory up to that point, which is a stacked matrix
                of all current entries, with zero-padding for empty entries.
                <div class="line-break"></div>

                The memory embedder is the final module in the memory pipeline. Its function is to take a given memory snapshot for a
                frame in the sequence and return an embedding that will ultimately be concatenated with the conditioning vector in the
                DiT. For our experiments, we tested two implementations of the embedder: a linear module which simply directly maps the
                memory snapshot matrix to an embedding, and a more involved memory transformer module (MiT).
                <div class="line-break"></div>
                <div class="line-break"></div>
                <div class="side-by-side-group">
                    <img src="67960BlogPost_files/mit.jpg" class="side-by-side-image" style="max-width: 500px"/>
                    <div class="caption">Architecture for the MiT memory embedder module.</div>
                </div>
                <div class="line-break"></div>
                <div class="line-break"></div>

                MiT takes in both the memory snapshot as well as a fixed-length portion of the most recent context frames as input. Both
                inputs are then embedded, tokenized, and fed to a standard non-causal transformer model that outputs the final memory
                embedding. We hypothesized that giving the memory embedder access to the past few frames would allow it to make better
                decisions when generating the final embedding.
                <h4>Benchmarking</h4>

                All of our experimentation kept our VAE frame encoder fixed. Our modified DiT was retrained for each experiment on our
                much smaller overfitting dataset. For every experiment, we kept our training process consistent at ~10k iterations. To
                evaluate the effectiveness of our different memory-conditioning approaches, we designed several challenging scenarios to
                benchmark the models. In each, we want to improve temporal consistency while still respecting the user input action
                stream.
                <div class="line-break"></div>

                The first scenario is “walk”. In walk, the user model is walking forward through a biome with a fixed forward-facing
                view angle. This tests the model’s ability to preserve terrain and biome information even with constant user motion and
                shows a particular configuration’s ability to achieve baseline usability in a scenario where memory shouldn’t be
                required for next-frame generation (since the frame context should always have similar-looking frames available).
                <div class="line-break"></div>

                Our second chosen scenario is “walk-frozen”. Walk-frozen is identical to walk, except we stop running the memory encoder
                after the initial sequence of prompt frames. This means that the memory bank always contains the same context snapshot.
                This evaluates the effectiveness of our memory bank’s eviction policy and determines whether the DiT is respecting our
                memory context without the possibility of polluting the memory bank with noisy values derived from generated frames.
                <div class="line-break"></div>

                The last and most challenging scenario we evaluate our models under is “sky”. In sky, we start with our memory bank
                filled with encodings of a user looking forwards into a given biome, but with its prompt frames looking straight upwards
                into the sky. Models that achieve strong temporal consistency should, when the action stream sends “look down” actions,
                generate frames that match the landscape and biome of the memory bank’s initial contents.
            </div>
            <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
            </div>
        </div>

        <!-- -->

        <div class="content-margin-container" id="experimentation_and_results">
            <div class="margin-left-block">
            </div>
            <div class="main-content-block">
                <h1>Experimentation and Results</h1>
                <h4>Baseline</h4>
                Our experimental control performed frame generation with our memory module disabled. We observed frame generations that
                were extremely temporally inconsistent in all situations. Walk exhibited dramatic landscape and biome shifts during our
                short generation window, and sky generated random-looking noise that seemed biased towards green colors and leaf-like
                textures (likely due to the prevalence of trees in our overfit dataset). These results were definitely partly due to our
                short training time, but this was expected as part of our goal was to improve consistency despite the short training
                time.
                <div class="line-break"></div>
                <div class="line-break"></div>
                <img src="67960BlogPost_files/gifs/results/better_baseline_walk.gif" class="side-by-side-image" />
                <div style="width: 100%; display: flex; align-items: center; justify-content: center; text-align: center;">
                    <div style="width: 15%;"></div>
                    <div class="caption">Ground truth - Walk</div>
                    <div style="width: 40%;"></div>
                    <div class="caption">Baseline - Walk</div>
                    <div style="width: 15%;"></div>
                </div>
                <div class="line-break"></div>
                <img src="67960BlogPost_files/gifs/results/baseline_sky.gif" class="side-by-side-image" />
                <div style="width: 100%; display: flex; align-items: center; justify-content: center; text-align: center;">
                    <div style="width: 15%;"></div>
                    <div class="caption">Ground truth - Sky</div>
                    <div style="width: 40%;"></div>
                    <div class="caption">Baseline - Sky</div>
                    <div style="width: 15%;"></div>
                </div>

                <h4>MiT + MC-Small</h4>
                The first experiment utilized our custom transformer-based memory embedder (MiT) and encoded the memory vectors using
                our high-compression memory compressor (MC-small). We pretrained the VAE used by MC-small similarly to our primary VAE
                using a large subset of the original dataset. MiT was included as part of the end-to-end DiT training process, as we
                hoped it would learn the memory embeddings without external training. However, in contrast to our lofty expectations for
                MiT, we observed results that were largely indistinguishable from the baseline. Even when we injected vastly different
                memories into the memory bank, our generation was unchanged. After researching this further, we believe that our DiT was
                learning to ignore our memory context due to MiT being trained concurrently and not providing useful input at early
                iterations <a href="#ref_8">[8]</a>. We tried rerunning this experiment without providing frame context to MiT in case the model was simply
                overfitting to that part of the input, however the results were unchanged.
                <div class="line-break"></div>
                <div class="line-break"></div>
                <img src="67960BlogPost_files/gifs/results/dit_mit_mim_10k_walk.gif" class="side-by-side-image" />
                <div style="width: 100%; display: flex; align-items: center; justify-content: center; text-align: center;">
                    <div style="width: 15%;"></div>
                    <div class="caption">Ground truth - Walk</div>
                    <div style="width: 40%;"></div>
                    <div class="caption">Experimental - Walk</div>
                    <div style="width: 15%;"></div>
                </div>
                <div class="line-break"></div>
                <img src="67960BlogPost_files/gifs/results/dit_mit_mim_10k_sky.gif" class="side-by-side-image" />
                <div style="width: 100%; display: flex; align-items: center; justify-content: center; text-align: center;">
                    <div style="width: 15%;"></div>
                    <div class="caption">Ground truth - Sky [Desert Memory]</div>
                    <div style="width: 26%;"></div>
                    <div class="caption">Experimental - Sky [Desert Memory]</div>
                    <div style="width: 28%;"></div>
                    <div class="caption">Experimental - Sky [Ocean Memory]</div>
                    <div style="width: 15%;"></div>
                </div>

                <h4>Linear + MC-Small</h4>
                After we decided that the outputs of MiT were likely being ignored by the DiT, we decided to employ a simple one-layer
                linear embedding of our memory snapshots. This change dramatically altered our diffusion outputs. For walk, we observed
                substantial improvement in temporal consistency. The frames better retained biome color palettes, as well as landscape
                features and smooth motion extrapolation, while still respecting the action stream. Walk-frozen was similar to walk, but
                noticeably biased toward preserving the landscape structure of the prompt frames while still walking the view model
                forward. This shows that the DiT is attending to the information in the memory bank, as it generates frames which match
                the prompt rather than just the frame context. Sky was dramatically improved, as the memory bank’s contents are
                respected when the user looks down rather than descending immediately into random noise, again showing that the memory
                bank’s contents are respected by the DiT. We did observe a second “look down” motion occurring occasionally during this
                step. We realized that this was likely due to our overfit DiT model deciding that it should look down before actually
                receiving the action specifying this, resulting in the double pivot effect seen in our sky generations.
                <div class="line-break"></div>
                <div class="line-break"></div>
                <img src="67960BlogPost_files/gifs/results/dit_linear_mim_10k_walk.gif" class="side-by-side-image" />
                <div style="width: 100%; display: flex; align-items: center; justify-content: center; text-align: center;">
                    <div style="width: 15%;"></div>
                    <div style="width: 30%;" class="caption">Ground truth - Walk</div>
                    <div style="width: 26%;"></div>
                    <div style="width: 30%;" class="caption">Experimental - Walk</div>
                    <div style="width: 28%;"></div>
                    <div style="width: 30%;" class="caption">Experimental - Walk<br/> [Frozen Embeddings]</div>
                    <div style="width: 15%;"></div>
                </div>
                <div class="line-break"></div>
                <img src="67960BlogPost_files/gifs/results/dit_linear_mim_10k_sky_1.gif" class="side-by-side-image" />
                <div style="width: 100%; display: flex; align-items: center; justify-content: center; text-align: center;">
                    <div style="width: 15%;"></div>
                    <div class="caption">Ground truth - Sky [Desert Memory]</div>
                    <div style="width: 26%;"></div>
                    <div class="caption">Experimental - Sky [Desert Memory]</div>
                    <div style="width: 28%;"></div>
                    <div class="caption">Experimental - Sky [Ocean Memory]</div>
                    <div style="width: 15%;"></div>
                </div>
                <div class="line-break"></div>
                <img src="67960BlogPost_files/gifs/results/dit_linear_mim_10k_sky_2.gif" class="side-by-side-image" />
                <div style="width: 100%; display: flex; align-items: center; justify-content: center; text-align: center;">
                    <div style="width: 15%;"></div>
                    <div class="caption">Experimental - Sky [Cherry Memory]</div>
                    <div style="width: 26%;"></div>
                    <div class="caption">Experimental - Sky [Dark Forest Memory]</div>
                    <div style="width: 28%;"></div>
                    <div class="caption">Experimental - Sky [Ice Memory]</div>
                    <div style="width: 15%;"></div>
                </div>

                <h4>Linear + MC-Large</h4>
                We designed our last experiment to evaluate the effects of compressing our memory entries. We set up our memory encoder
                (MC-large) to directly output latents from the primary VAE model, and thus our memory bank simply acted as an ‘extended’
                frame context. This was a control to see if the large-scale compression performed by MC-small was meaningful, or if
                simply ‘extending’ the frame context would be more effective (albeit at the cost of a higher memory footprint).
                <div class="line-break"></div>
                For walk, we observed slightly better than baseline terrain structure preservation but the biome and color palette of
                the frame prompt was quickly lost. Walk-frozen preserved some fine-grained details like block outlines that were lost in
                walk, but was also unable to preserve color information for an extended period of time. Sky was difficult to distinguish
                from baseline and did not appropriately react to differing memory contents, indicating that the performance of our
                uncompressed memory was highly compromised.

                <div class="line-break"></div>
                <div class="line-break"></div>
                <img src="67960BlogPost_files/gifs/results/dit_linear_mim_fullres_8k_walk.gif" class="side-by-side-image" />
                <div style="width: 100%; display: flex; align-items: center; justify-content: center; text-align: center;">
                    <div style="width: 15%;"></div>
                    <div style="width: 30%;" class="caption">Ground truth - Walk</div>
                    <div style="width: 26%;"></div>
                    <div style="width: 30%;" class="caption">Experimental - Walk</div>
                    <div style="width: 28%;"></div>
                    <div style="width: 30%;" class="caption">Experimental - Walk<br/> [Frozen Embeddings]</div>
                    <div style="width: 15%;"></div>
                </div>
                <div class="line-break"></div>
                <img src="67960BlogPost_files/gifs/results/dit_linear_mim_fullres_8k_sky.gif" class="side-by-side-image" />
                <div style="width: 100%; display: flex; align-items: center; justify-content: center; text-align: center;">
                    <div style="width: 15%;"></div>
                    <div class="caption">Ground truth - Sky [Desert Memory]</div>
                    <div style="width: 26%;"></div>
                    <div class="caption">Experimental - Sky [Desert Memory]</div>
                    <div style="width: 28%;"></div>
                    <div class="caption">Experimental - Sky [Ocean Memory]</div>
                    <div style="width: 15%;"></div>
                </div>
            </div>
            <div class="margin-right-block">
            </div>
        </div>

        <!-- -->

        <div class="content-margin-container" id="implications_and_limitations">
            <div class="margin-left-block">
            </div>
            <div class="main-content-block">
                <h1>Implications and Limitations</h1>
                Our experiments show that including a selectively-updated memory context in Oasis’ latent diffusion successfully visibly
                improved the temporal consistency during both consistent and inconsistent frame streams. The memory context was clearly
                respected by the DiT’s generations (as shown by the walk-frozen experiments) and resulted in drastic qualitative
                improvement even with our relatively short diffusion training process.
                <div class="line-break"></div>
                The results of our experiments also indicate that our memory context is actually more performant with our memory
                compressor (MC) enabled. This is significant as compression massively reduces the size of our memory bank and the
                objective is to increase its “length” as much as possible while keeping our model’s overall memory size small and fast
                to train. In addition, we hypothesize that the compressed memory encoding reduces noise and helps our linear embedding
                layer to easily synthesize our memory snapshot into the small dimension of the DiT latent conditioning vector.
                <div class="line-break"></div>
                We believe that the promising results of our memory augmentation to Oasis suggests great utility for selectively-updated
                memory in various video-generation use cases. Our model uses greatly simplified approaches for tasks such as memory
                compression, replacement, encoding, and conditioning. With additional time and resources, we believe that future work
                could train simple models to further enhance the impact of an external memory while maintaining the benefits conferred
                by our memory implementation. We believe that if the MiT were trained with a reconstruction loss on the full video
                dataset the DiT may actually see the benefit of a more complex memory encoding. Regardless, any fully-trained DiT should
                see at least some benefit from including a memory module such as ours thanks to our compression and latent encoding
                steps.
            </div>
            <div class="margin-right-block">
            </div>
        </div>

        <!-- -->

        <div class="content-margin-container" id="citations">
            <div class="margin-left-block">
            </div>
            <div class="main-content-block">
                <div class="citation" id="references" style="height:auto"><br>
                    <h1>Citations:</h1>
                    <a id="ref_1"></a>[1] Decart, J. Quevedo, Q. McIntyre, S. Campbell, R. Wachen, and Etched, “Oasis.” [Online]. Available:
                    <a href="https://oasis-model.github.io/">https://oasis-model.github.io/</a><br><br>
                    <a id="ref_2"></a>[2] D. Valevski, Y. Leviathan, M. Arar, and S. Fruchter, “Diffusion Models are Real-Time Game
                    Engines,” [Online]. Available: <a href="https://arxiv.org/pdf/2408.14837">https://arxiv.org/pdf/2408.14837</a><br><br>
                    <a id="ref_3"></a>[3] D. Ha and J. Schmidhuber, “World Models.” [Online]. Available: <a
                        href="https://arxiv.org/pdf/2408.14837">https://arxiv.org/pdf/2408.14837</a><br><br>
                    <a id="ref_4"></a>[4] B. Chen, D. M. Monso, Y. Du, M. Simchowitz, R. Tedrake, and V. Sitzmann, “Diffusion Forcing:
                    Next-token Prediction Meets Full-Sequence Diffusion.” [Online]. Available: <a
                        href="https://arxiv.org/abs/2407.01392">https://arxiv.org/abs/2407.01392</a><br><br>
                    <a id="ref_5"></a>[5] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The Unreasonable Effectiveness of
                    Deep Features as a Perceptual Metric,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE,
                    Jun. 2018. [Online]. Available: <a
                        href="https://doi.org/10.1109/cvpr.2018.00068">https://doi.org/10.1109/cvpr.2018.00068</a><br><br>
                    <a id="ref_6"></a>[6] B. Baker et al., “Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos,”
                    OpenAI. [Online]. Available: <a href="https://arxiv.org/abs/2206.11795">https://arxiv.org/abs/2206.11795</a><br><br>
                    <a id="ref_7"></a>[7] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-Resolution Image Synthesis with
                    Latent Diffusion Models.” [Online]. Available: <a
                        href="https://arxiv.org/abs/2112.10752">https://arxiv.org/abs/2112.10752</a><br><br>
                    <a id="ref_8"></a>[8] “jbetker,” “Surrogate Losses for Diffusion Models – Non_Interactive – Software & ML.” [Online].
                    Available: <a href="https://nonint.com/2022/04/04/190/">https://nonint.com/2022/04/04/190/</a><br><br>
                    <a id="ref_9"></a>[9] J. Song, C. Meng, and S. Ermon, “Denoising Diffusion Implicit Models.” [Online]. Available: <a
                        href="https://arxiv.org/abs/2010.02502">https://arxiv.org/abs/2010.02502</a><br><br>
                </div>
            </div>
            <div class="margin-right-block">
                <!-- margin notes for reference block here -->
            </div>
        </div>
    </body>
</html>
